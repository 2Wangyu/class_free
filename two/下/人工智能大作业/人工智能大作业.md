# 代码的详解

这段代码是一个使用 PaddlePaddle 框架处理并扩展 MNIST 数据集的示例，目的是为了创建一个包含原始和颜色反转图像的新数据集，这对于提高模型对不同背景条件下的泛化能力可能是有帮助的。具体来说，代码的主要功能和组成部分如下：

1. **颜色反转函数 (`invert_colors`)**:
    - 这个函数通过将像素值从原始图像中减去，从而实现黑白颜色的反转。这里的实现可能有误，因为MNIST图像的像素值范围是从0到255，所以正确的反转应该是 `255 - img` 而不是 `0 - img`。

2. **创建训练和测试数据集**:
    - 代码通过遍历原始的 `train_dataset` 和 `test_dataset`，将每个图像及其标签加入新列表，并对每个原始图像进行颜色反转，反转后的图像和原始标签也加入相应列表。
    - 这样做会使得新数据集包含等量的原始图像和反转图像，且每对图像都共享相同的标签。

3. **自定义 PaddlePaddle 数据集 (`CustomMNISTDataset`)**:
    - 定义了一个继承自 `Dataset` 的类，用于封装图像和标签列表，以便于使用 PaddlePaddle 进行训练和测试。
    - 实现了 `__getitem__` 方法以返回特定索引的图像和标签，`__len__` 方法以返回数据集的大小。

4. **初始化并使用自定义数据集**:
    - 使用处理过的图像和标签列表实例化 `CustomMNISTDataset` 类，创建了训练和测试数据集的对象。

### 改进建议

1. **颜色反转函数**:
   ```python
   def invert_colors(img):
       return 255 - img
   ```
   这将正确反转图像的颜色，使黑色变为白色，反之亦然。

2. **数据类型和形状**:
   - 确保图像数据在进行颜色反转前后都保持正确的数据类型（如 `np.uint8`）和形状，这对于后续的图像处理和模型训练至关重要。
   - 如果使用深度学习框架，通常需要调整图像形状以匹配模型输入，比如增加一个维度表示颜色通道。

3. **数据可视化**:
   - 在实际应用之前，可视化一些反转后的图像和原始图像，以确保数据处理的正确性。

通过这些步骤，可以有效地准备并验证处理后的数据集，为使用深度学习模型进行训练和测试奠定基础。













1. 卷积层：模型包含两个卷积层。

2. 池化层

   ：每个卷积层后面跟着一个最大池化层。

   - 池化核大小为2x2，步长为2。

3. 全连接层

   ：

   - 第一个全连接层将卷积层输出的特征图展平后的维度（16*5*5）连接到120个神经元。
   - 第二个全连接层从120个神经元连接到84个神经元。
   - 最后一个全连接层从84个神经元连接到10个神经元，对应10个分类输出。



原始的Lenet5

原始28X28X1

- 第一层卷积层使用了6个滤波器（输出通道数），滤波器大小为5x5X1（尺寸为5X5深度为1），步长（S）为1，边界(P)填充为2。

     输入：32X32X1

     输出：28X28X6

  

  池化层： 池化核大小为2x2，步长为2

  ​     输出14X14X6

  平均池化层：

  

- 第二层卷积层使用了16个滤波器，滤波器大小为5x5，步长为1，无边界填充。

  输入：14X14X6

  输出： 10X10X16



池化层：同样的

输出5X5X16





- 第一个全连接层将卷积层输出的特征图展平后的维度（16）连接到120个神经元，5X5X16。

  输出1X1X120

- 第二个全连接层从120个神经元连接到84个神经元  1X1X1

​       输出 1X1X84

- 最后一个全连接层从84个神经元连接到10个神经元，对应10个分类输出

​       RBF





修改relu函数，最大池化层







### 特定设置

- **`LeNet_relu`**：在所有激活点使用ReLU函数（修正线性单元）作为激活函数。
- **`LeNet_sigmoid`**：使用Sigmoid函数作为激活函数。

### 正向传播（Forward Pass）

- 数据通过两个卷积层，每个卷积层后接ReLU或Sigmoid激活函数和一个最大池化层。
- 经过卷积和池化处理后的数据被展平，然后通过三个全连接层，全连接层间使用相应的激活函数。
- 最终输出层在`LeNet_relu`类中没有显式的激活函数，而`LeNet_sigmoid`类中使用的是Sigmoid函数，但一般分类任务最后一层常使用Softmax，这可能是一个遗漏或特定设计选择。







vgg的学习率不能超过0.002

必须用卡去跑，同时不能使用传统的VGG,需要对网络做出修改。





Lenet5

# **数据集规模，训练集、(验证集)、测试集的样本分配**

数据规模：70000个样本数据

​    原始数据集60,000 个训练样本和 10,000 个测试样本

原始数据集：28X28X1

 

 

# **模型的网络结构:几层卷积层、几个卷积、卷积大小、激活函数、损失函数、池化层、全连接层、输出层几个神经元**

 

 

 

损失函数:

使用的是 paddle.nn.CrossEntropyLoss()。这个损失函数特别适用于分类问题，它计算的是交叉熵损失。

交叉熵损失是一个常用的损失函数，尤其是在处理多类分类问题时。它衡量的是模型输出的概率分布与目标标签的真实分布之间的差异。在数学上，对于多分类问题，交叉熵损失可以表示为：
$$
L=-\sum_{c=1}^{M} y_{o, c} \operatorname{l o g} ( p_{o, c} )
$$
 这里：

- *M* 是类别的数量。
- *y* 是二进制指示向量（只有正确类别的位置是1，其它都是0）。
- *p* 是模型预测每个类别的概率。
- *o* 是样本的索引。

通过最小化这个损失值，你可以让模型在训练过程中逐渐提高准确性，即模型输出的概率分布越来越接近实际的标签分布。



### 1. Epoch

**Epoch**指的是将整个训练数据集完整地传递给学习算法的次数。在每个epoch中，模型都会尝试学习数据中的特征并进行参数更新，以减少误差。Epoch的影响包括：

- **过少的Epochs**：如果epoch数过少，模型可能没有足够的时间学习数据，导致**欠拟合**。欠拟合的模型在训练数据和未见过的新数据上都表现不佳。
- **过多的Epochs**：相反，如果epoch数过多，模型可能会学习到数据中的噪声和非代表性特征，导致**过拟合**。过拟合的模型在训练数据上表现良好，但在新数据上表现较差。
- **调节方法**：通常需要通过实验或使用如早停（early stopping）等技术来确定合适的epoch数量。早停是一种正则化方法，当验证集的性能不再提高时停止训练。

### 2. Batch Size

**Batch Size**是指在训练网络时每次传递给网络的数据样本数量。它影响模型的训练速度和性能：

- 较小的Batch Size

  - 可以提供更精确的估计梯度，但估计结果会有较高的方差，可能导致训练过程中出现较大波动。
  - 通常能帮助模型跳出局部最小值，可能获得更好的训练结果。
  - 需要更多的更新步骤，训练时间可能更长。

- 较大的Batch Size

  ：

  - 提供的梯度估计的方差较低，但可能陷入局部最小值。
  - 可以利用现代硬件（如GPU）的并行处理能力，加快训练速度。
  - 可能需要调整学习率和其他优化参数来达到最佳训练效果。

- **调节方法**：选择合适的batch size通常需要考虑可用的计算资源、内存限制和模型的具体需求。通过调整batch size并观察模型在验证集上的表现来找到最优值。

在深度学习训练过程中，`epoch`和`batch size`的典型设置可以根据任务的复杂性、数据集的大小以及使用的硬件资源有很大的差异。不过，有一些常见的基准值和实践可以参考：

### Epochs

Epoch的数量依赖于模型对数据的学习速率和数据集的大小。较大的数据集可能不需要太多的epochs，因为模型有更多的数据点来学习，减少了过拟合的风险。常见的设置包括：

- **小到中等数据集**：通常设置在10到100个epochs之间。如果数据集较小，可能需要更多的epochs来帮助模型足够学习到数据的特征。

- **大型数据集**：可能只需要几个epochs，甚至一个epoch就足够，尤其是在使用复杂的模型结构时。

- **实际应用中**：经常结合早停技术（early stopping）来决定epoch的数量。早停方法可以监控验证集的性能，当性能在连续几个epoch后不再提升时停止训练，这有助于防止过拟合。

- `batch_size` 是深度学习和机器学习中的一个重要参数，它决定了模型在训练过程中每次迭代（iteration）所使用的样本数量。下面我将详细解释 `batch_size` 的含义和其对模型训练的影响。

  # batch_size

  ### 定义

  在训练神经网络时，我们通常不会一次性使用整个数据集来计算梯度并更新模型的权重，而是将数据分成多个较小的子集（称为“批次”或“mini-batches”）来迭代地进行训练。每个这样的子集的大小就是 `batch_size`。

  ### 示例

  假设你有一个包含 1000 个样本的数据集，并且你设置了 `batch_size` 为 100。那么，在每次迭代中，你将使用 100 个样本来计算损失函数的梯度，并据此更新模型的权重。整个数据集将被分为 10 个批次进行处理，因此你需要进行 10 次迭代才能遍历完整个数据集（这被称为一个“epoch”）。

  ### 对模型训练的影响

  1. **计算效率**：使用较大的 `batch_size` 可以提高计算效率，因为你可以同时处理更多的样本，从而更充分地利用计算资源（如 GPU）。然而，这也需要更多的内存来存储每个批次的输入和输出。
  2. **梯度估计**：使用较大的 `batch_size` 通常可以提供更准确的梯度估计，因为梯度是基于更多的样本来计算的。然而，这也可能导致模型在训练过程中过于依赖当前的批次，从而降低了模型的泛化能力。
  3. **收敛速度**：在训练初期，较大的 `batch_size` 通常可以加速模型的收敛，因为每次迭代都使用了更多的样本来更新权重。然而，在训练后期，过大的 `batch_size` 可能导致模型在最优解附近震荡，而无法进一步精细调整权重。
  4. **内存占用**：`batch_size` 的大小直接影响训练过程中的内存占用。较大的 `batch_size` 需要更多的内存来存储输入数据、输出数据和中间计算结果。因此，在选择 `batch_size` 时，需要考虑到可用的计算资源。
  5. **正则化效果**：较小的 `batch_size` 可以带来一种类似于正则化的效果，因为每个批次都包含了数据集中的一部分随机样本。这种随机性有助于防止模型过度拟合到特定的训练数据。

  ### 如何选择

  在选择 `batch_size` 时，通常需要根据具体的任务、数据集和计算资源来进行权衡。较大的 `batch_size` 可以提高计算效率，但可能牺牲一定的泛化能力；而较小的 `batch_size` 则可以提供更好的泛化能力，但可能需要更多的迭代次数才能达到相同的收敛水平。此外，还可以使用一些策略来动态调整 `batch_size`，以在训练过程中实现更好的性能。

### Batch Size

Batch size 的选择对模型的性能和训练速度有直接影响。较大的batch size可以加快训练速度并提高内存利用率，但可能影响模型的最终性能，特别是在梯度更新的准确性方面：

- **小到中等规模的任务**：常见的batch size设置为32、64或128。这些值是经验性的选择，可以提供良好的性能和训练速度的平衡。
- **在使用GPU的情况下**：由于并行处理能力，可以使用更大的batch size，如256、512甚至更高。但应注意，过大的batch size可能需要调整学习率和其他优化参数。
- **对于大型数据集或复杂模型**：较大的batch size（例如1024以上）有时被用来处理极大规模的数据集，如在处理图像或视频时。

### 调整建议

- **经验法则**：开始实验时可以使用中等大小的batch size（如32或64），根据模型在验证集上的性能和训练时间逐渐调整。
- **监控**：始终监控训练和验证损失以及准确率，以评估当前设置的效果。
- **资源限制**：实际的batch size可能受到系统内存和计算能力的限制，因此在不牺牲太多性能的情况下尽可能使用最大可行的batch size。

调整这些参数时，应考虑到模型的具体需求和训练环境，通常需要通过一系列试验来确定最优的设置。

`batch-size`（批量大小）是机器学习训练过程中的一个重要参数，它决定了在每次模型权重更新之前，网络会查看多少个样本。`batch-size`的选择对训练过程、模型的准确率（accuracy）和精度（precision）都有一定的影响，但这种影响并不是直接的，而是间接的，并通过以下几个方面体现：

1. 计算效率

   ：

   - 较大的`batch-size`通常意味着更高效的GPU利用率，因为可以并行处理更多的数据。这有助于减少计算时间，但可能会增加内存需求。
   - 过小的`batch-size`可能导致GPU利用率低下，因为需要频繁地进行权重更新，而每次更新只涉及少量的数据。

2. 泛化能力

   ：

   - 较大的`batch-size`可能会使模型在训练集上的性能更好（即训练误差更低），但可能导致在测试集上的性能较差（即泛化能力较差）。这是因为大批量可能无法充分捕捉到数据中的多样性。
   - 较小的`batch-size`（如使用随机梯度下降SGD或小批量梯度下降Mini-batch SGD）通常具有更好的泛化能力，因为它们能够在每次更新时暴露给模型更多的数据多样性。

3. 收敛稳定性

   ：

   - 较大的`batch-size`可能导致优化过程中的损失函数波动较小，因为每次更新都基于更多的数据。这有助于稳定的收敛，但也可能导致模型陷入局部最优解。
   - 较小的`batch-size`可能导致损失函数在训练过程中波动较大，但这有助于模型探索更多的参数空间，并可能找到更好的全局最优解。

4. 正则化效果

   ：

   - 在某种程度上，使用较小的`batch-size`（特别是与随机梯度下降SGD结合使用时）具有类似于正则化的效果，因为它增加了模型参数更新的噪声。这种噪声有助于防止过拟合。

5. 学习率调整

   ：

   - 不同的`batch-size`可能需要不同的学习率调整策略。较大的`batch-size`通常可以容忍较大的学习率，而较小的`batch-size`可能需要较小的学习率以防止训练不稳定。

6. 准确率与精度

   ：

   - 准确率（accuracy）和精度（precision）是评估模型性能的指标。它们受到`batch-size`的间接影响，因为`batch-size`的选择会影响模型的训练过程、泛化能力和收敛稳定性。但是，这些指标与`batch-size`之间并没有直接的数学关系。

总之，选择合适的`batch-size`是一个权衡各种因素的过程，包括计算效率、泛化能力、收敛稳定性和正则化效果等。在实际应用中，通常需要通过实验来确定最佳的`batch-size`。





在深度学习和数据科学领域，“小型”、“中等”和“大型”数据集的定义可以根据具体的应用、算法的复杂性和处理能力而变化。以下是一些大致的指南，以帮助划分这些类别：

### 小型数据集
- **大小**：通常包含数百到几千个样本。
- **用途**：适合初步测试、原型设计或在计算资源有限的情况下使用。
- **示例**：在机器学习教程或小规模竞赛中常见的数据集，如UCI机器学习仓库中的数据集。

### 中等数据集
- **大小**：通常在几千到十万个样本之间。
- **用途**：足以用于商业应用和科研，可以展示某些算法的有效性而不至于计算过于复杂。
- **示例**：很多常用的基准数据集，如MNIST手写数字识别（约7万个样本）、CIFAR-10（6万个图像）等。

### 大型数据集
- **大小**：从几十万到数百万甚至数十亿个样本。
- **用途**：用于工业级应用和大规模科研，需要强大的计算资源和精细的算法优化。
- **示例**：ImageNet数据集（超过1400万张图像）、OpenAI的GPT训练数据集（数十亿个标记的文本数据）、大型电子商务或社交媒体平台生成的用户交互数据等。

### 实际应用
在实际应用中，数据集的“大型”和“小型”也取决于所用技术的处理能力和算法的效率。例如，对于一些高级深度学习模型，即使是数万个样本也可能被认为是较小的数据集，因为这些模型通常需要大量数据才能表现出优越的学习能力。相反，在一些传统的统计学习任务中，几千个样本就可能足够了。

### 数据集选择的考虑因素
选择数据集时，除了数量级外，还应考虑数据的质量、多样性和代表性，这些因素同样重要。一个小但高质量和多样性的数据集可能比一个大但低质量或偏差严重的数据集更有价值。因此，数据集的选择和使用应根据实际的业务需求、研究目标和可用资源综合考虑。





# 学习率的调参

是机器学习模型训练过程中的一个重要环节。学习率决定了模型在训练过程中参数更新的步长，对模型的训练速度和性能有显著影响。以下是关于学习率调参的一些关键点和建议：

1. 初始学习率的选择

   ：

   - 通常，学习率的初始值需要根据具体任务和模型架构来设定。较大的初始学习率可以使模型更快地收敛，但也可能导致训练不稳定甚至发散。
   - 在深度学习模型中，常见的初始学习率范围通常在0.001到0.1之间。然而，这只是一个大致的范围，具体值需要根据实验来确定。

2. 学习率的调整策略

   - 固定学习率：在训练过程中保持学习率不变。这种方法简单直接，但可能不适用于所有情况。
   - 逐步衰减的学习率：随着训练的进行，逐渐减小学习率。这有助于模型在训练后期更加精细地调整参数，避免在最优解附近震荡。常见的衰减策略包括指数衰减、多项式衰减等。
   - 自适应学习率：根据训练过程中的某些指标（如梯度、损失函数的值等）自动调整学习率。这种方法可以使学习率更加灵活，但也可能增加训练的复杂性。

3. 学习率调参的注意事项

   ：

   - 监控训练过程：在训练过程中，需要密切关注模型的性能（如准确率、损失函数的值等）以及训练稳定性（如梯度是否过大、是否出现震荡等）。这些指标可以帮助我们判断当前的学习率是否合适。
   - 尝试不同的学习率：在训练初期，可以尝试使用不同的学习率进行训练，观察模型的性能变化。通过对比不同学习率下的训练效果，可以选择一个较为合适的学习率。
   - 结合其他超参数调整：学习率的调整通常需要与其他超参数（如批处理大小、正则化系数等）相结合。因此，在调参时需要考虑它们之间的相互影响。

4. 使用调参工具

   ：

   - 可以利用一些专门的调参工具（如Hyperopt、GridSearch等）来自动搜索最佳的学习率和其他超参数组合。这些工具可以大大节省调参的时间和精力。

总之，学习率的调参是机器学习模型训练过程中的一个重要环节。通过合理选择初始学习率、采用适当的调整策略和注意事项，并结合其他超参数的调整，可以使模型达到更好的性能。

学习率（Learning Rate）在机器学习模型的训练过程中起着至关重要的作用，它直接决定了模型权重更新的步长。学习率对模型精准度的影响是一个复杂且多面的过程，下面将分点介绍学习率提高精准度的变化及其相关因素：

1. 学习率与精准度的关系

   ：

   - **学习率过大**：如果学习率设置得过大，模型权重更新的步长会很大，这可能导致模型在训练过程中跳过最优解，导致模型无法收敛，从而降低精准度。
   - **学习率过小**：相反，如果学习率设置得过小，模型权重更新的步长会很小，虽然模型可能会收敛到最优解，但收敛速度会非常慢，且容易陷入局部最优解，同样会降低精准度。
   - **合适的学习率**：寻找一个合适的学习率对于提高模型精准度至关重要。通过不断调整学习率，并观察模型在验证集上的性能变化，可以找到最优的学习率，从而提高模型的精准度。

2. 学习率与训练轮数（Epochs）的关系

   ：

   - 在模型训练过程中，随着训练轮数的增加，模型逐渐学习到数据的特征。合适的学习率可以帮助模型在更少的轮数内达到更高的精准度。
   - 如果学习率设置得过大，模型可能在较少的轮数内就达到较高的精准度，但可能会过拟合训练数据，导致在测试集上的性能下降。
   - 如果学习率设置得过小，模型可能需要更多的轮数才能达到较高的精准度，且容易陷入局部最优解。

3. 学习率与批量大小（Batch Size）的关系

   ：

   - 批量大小决定了每次权重更新时使用的样本数量。较大的批量大小通常可以使用较大的学习率，因为更多的样本可以提供更准确的梯度估计。
   - 相反，较小的批量大小可能需要使用较小的学习率，以避免在权重更新时产生过大的波动。

4. 学习率调整策略

   ：

   - 在训练过程中，学习率通常不会保持不变。一种常见的策略是在训练初期使用较大的学习率以加快收敛速度，然后在训练后期逐渐减小学习率以提高模型在验证集上的性能。
   - 还可以使用一些自适应学习率算法，如Adam、RMSprop等，这些算法可以根据模型的训练情况自动调整学习率。

**总结**：
学习率是影响机器学习模型精准度的重要参数之一。通过调整学习率的大小和变化策略，可以在一定程度上提高模型的精准度。然而，最佳的学习率通常需要根据具体的任务和数据集进行调整和实验。在实际应用中，建议通过网格搜索、随机搜索等方法来寻找最优的学习率组合。



## 修改加深网络

基于您提供的`LeNet_relu_deepened`类，以下是每一层的卷积层、池化层、全连接层的排布以及它们的输入输出描述：

第三个卷积层 (conv3):输入通道数: 16（来自上一个卷积层的输出）输出通道数: 32卷积核大小: 3x3步长: 1填充: 1这层之后跟着一个最大池化层，池化窗口为2x2，步长为2。这样的设置有助于进一步减小特征图的空间尺寸，增加特征的抽象程度。第四个卷积层 (conv4):输入通道数: 32（来自上一个卷积层的输出）输出通道数: 64卷积核大小: 3x3步长: 1填充: 1与传统的LeNet-5不同，此层后面没有池化层。保留较大的特征图尺寸，可以为随后的全连接层提供更丰富的特征。

### 卷积层

1. conv1
   - 输入: 1通道, 任意大小（但通常是32x32）的图像
   - 输出: 6通道, (原始宽度-4)/1+1, (原始高度-4)/1+1 的特征图（由于padding=2和kernel_size=5）
   - 参数: 6个5x5的卷积核，每个卷积核有1个输入通道和6个输出通道
2. conv2
   - 输入: 6通道, (conv1输出宽度)/2, (conv1输出高度)/2 的特征图（由于max_pool1的池化）
   - 输出: 16通道, (conv2输入宽度-4)/1+1, (conv2输入高度-4)/1+1 的特征图
   - 参数: 16个5x5的卷积核，每个卷积核有6个输入通道和16个输出通道
3. conv3
   - 输入: 16通道, (conv2输出宽度)/2, (conv2输出高度)/2 的特征图（由于max_pool2的池化）
   - 输出: 32通道, (conv3输入宽度-2)/1+1, (conv3输入高度-2)/1+1 的特征图（由于padding=1和kernel_size=3）
   - 参数: 32个3x3的卷积核，每个卷积核有16个输入通道和32个输出通道
4. conv4
   - 输入: 32通道, (conv3输出宽度)/2, (conv3输出高度)/2 的特征图（由于max_pool3的池化）
   - 输出: 64通道, (conv4输入宽度-2)/1+1, (conv4输入高度-2)/1+1 的特征图
   - 参数: 64个3x3的卷积核，每个卷积核有32个输入通道和64个输出通道

### 池化层

1. max_pool1
   - 输入: conv1的输出
   - 输出: (conv1输出宽度)/2, (conv1输出高度)/2 的特征图
   - 参数: 2x2的池化窗口，步长为2
2. max_pool2
   - 输入: conv2的输出
   - 输出: (conv2输出宽度)/2, (conv2输出高度)/2 的特征图
   - 参数: 2x2的池化窗口，步长为2
3. max_pool3
   - 输入: conv3的输出
   - 输出: (conv3输出宽度)/2, (conv3输出高度)/2 的特征图
   - 参数: 2x2的池化窗口，步长为2

### 全连接层

1. linear1
   - 输入: conv4输出特征图展平后的向量，大小为 64 * (conv4输出宽度) * (conv4输出高度)
   - 输出: 120个神经元的输出
   - 参数: 输入层有 64 * (conv4输出宽度) * (conv4输出高度) 个神经元，输出层有120个神经元
2. linear2
   - 输入: linear1的输出，即120个神经元的输出
   - 输出: 84个神经元的输出
   - 参数: 输入层有120个神经元，输出层有84个神经元
3. linear3
   - 输入: linear2的输出，即84个神经元的输出
   - 输出: 10个神经元的输出（通常对应于10个分类的输出）
   - 参数: 输入层有84个神经元，输出层有10个神经元（如果是多分类任务）







# **超参个数、如何设置的(学习率等)**

 

# **训练模型的过程、训练时长**

 

# **是否修改了网络结构，怎么修改的**

 

# **尝试了几个模型，对比结果的结论是什么**

可以准备一个Excel表格，展示对比结果，包括所用的模型名称、不同的优化器、超参设置及相应的precision、recall等

#  

# **展示运行结果**

 





